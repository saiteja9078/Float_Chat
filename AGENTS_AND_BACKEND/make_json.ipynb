{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ba52eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to process Argo float data...\n",
      "Found 21 float directories\n",
      "Processing float: 1902767\n",
      "Processing float: 1902677\n",
      "Processing float: 2902114\n",
      "Processing float: 1902670\n",
      "Processing float: 2900229\n",
      "Processing float: 2900228\n",
      "Processing float: 1902671\n",
      "Processing float: 2900226\n",
      "Processing float: 1902676\n",
      "Processing float: 1900121\n",
      "Processing float: 5907092\n",
      "Processing float: 2900232\n",
      "Processing float: 2900233\n",
      "Processing float: 1902673\n",
      "Processing float: 1902674\n",
      "Processing float: 1902675\n",
      "Processing float: 1900122\n",
      "Processing float: 1902672\n",
      "Processing float: 2900230\n",
      "Processing float: 1902669\n",
      "Processing float: 1902785\n",
      "Data saved to argo_floats_data.json\n",
      "Processing complete! Found 21 floats.\n",
      "\n",
      "Float 1902767:\n",
      "  Location: Indian Ocean\n",
      "  Cycles: 19\n",
      "  Temperature range: 2.71 - 17.28\n",
      "  Salinity range: 34.27 - 35.53\n",
      "\n",
      "Float 1902677:\n",
      "  Location: Indian Ocean\n",
      "  Cycles: 61\n",
      "  Temperature range: 2.37 - 30.86\n",
      "  Salinity range: 33.62 - 35.31\n",
      "\n",
      "Float 2902114:\n",
      "  Location: Bay of Bengal\n",
      "  Cycles: 284\n",
      "  Temperature range: 2.51 - 31.84\n",
      "  Salinity range: 23.69 - 35.20\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ArgoFloatProcessor:\n",
    "    def __init__(self, data_directory):\n",
    "        self.data_directory = Path(data_directory)\n",
    "        self.float_data = {}\n",
    "        \n",
    "    def determine_location(self, lat, lon):\n",
    "        \"\"\"Determine ocean region based on latitude and longitude\"\"\"\n",
    "        if pd.isna(lat) or pd.isna(lon):\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        # Bay of Bengal: roughly 5-22°N, 80-100°E\n",
    "        if 5 <= lat <= 22 and 80 <= lon <= 100:\n",
    "            return \"Bay of Bengal\"\n",
    "        \n",
    "        # Arabian Sea: roughly 0-30°N, 50-80°E\n",
    "        elif 0 <= lat <= 30 and 50 <= lon <= 80:\n",
    "            return \"Arabian Sea\"\n",
    "        \n",
    "        # Indian Ocean: broader region\n",
    "        elif -50 <= lat <= 30 and 20 <= lon <= 120:\n",
    "            return \"Indian Ocean\"\n",
    "        \n",
    "        else:\n",
    "            return \"Other\"\n",
    "    \n",
    "    def safe_float_conversion(self, value):\n",
    "        \"\"\"Safely convert value to float, return NaN if not possible\"\"\"\n",
    "        try:\n",
    "            if pd.isna(value):\n",
    "                return float('nan')\n",
    "            return float(value)\n",
    "        except (ValueError, TypeError):\n",
    "            return float('nan')\n",
    "    \n",
    "    def safe_str_conversion(self, value):\n",
    "        \"\"\"Safely convert value to string\"\"\"\n",
    "        try:\n",
    "            if pd.isna(value):\n",
    "                return \"\"\n",
    "            if isinstance(value, bytes):\n",
    "                return value.decode('utf-8').strip()\n",
    "            return str(value).strip()\n",
    "        except:\n",
    "            return \"\"\n",
    "    \n",
    "    def calculate_stats(self, data):\n",
    "        \"\"\"Calculate min, max, avg from data array\"\"\"\n",
    "        try:\n",
    "            if data is None or len(data) == 0:\n",
    "                return float('nan'), float('nan'), float('nan')\n",
    "            \n",
    "            # Flatten the data if it's multi-dimensional\n",
    "            flat_data = np.array(data).flatten()\n",
    "            \n",
    "            # Remove NaN values\n",
    "            valid_data = flat_data[~np.isnan(flat_data)]\n",
    "            \n",
    "            if len(valid_data) == 0:\n",
    "                return float('nan'), float('nan'), float('nan')\n",
    "            \n",
    "            return float(np.min(valid_data)), float(np.max(valid_data)), float(np.mean(valid_data))\n",
    "        except:\n",
    "            return float('nan'), float('nan'), float('nan')\n",
    "    \n",
    "    def process_meta_file(self, file_path, float_id):\n",
    "        \"\"\"Process meta.nc file\"\"\"\n",
    "        try:\n",
    "            with xr.open_dataset(file_path) as ds:\n",
    "                meta_info = {\n",
    "                    'platform_number': self.safe_str_conversion(ds.PLATFORM_NUMBER.values),\n",
    "                    'wmo_inst_type': self.safe_str_conversion(ds.WMO_INST_TYPE.values),\n",
    "                    'project_name': self.safe_str_conversion(ds.PROJECT_NAME.values),\n",
    "                    'pi_name': self.safe_str_conversion(ds.PI_NAME.values),\n",
    "                    'data_centre': self.safe_str_conversion(ds.DATA_CENTRE.values),\n",
    "                    'launch_info': {\n",
    "                        'date': self.safe_str_conversion(ds.LAUNCH_DATE.values),\n",
    "                        'latitude': self.safe_float_conversion(ds.LAUNCH_LATITUDE.values),\n",
    "                        'longitude': self.safe_float_conversion(ds.LAUNCH_LONGITUDE.values),\n",
    "                        'platform_type': self.safe_str_conversion(ds.PLATFORM_TYPE.values),\n",
    "                        'float_serial_no': self.safe_str_conversion(ds.FLOAT_SERIAL_NO.values),\n",
    "                        'deployment_platform': self.safe_str_conversion(ds.DEPLOYMENT_PLATFORM.values) if 'DEPLOYMENT_PLATFORM' in ds else \"\",\n",
    "                        'deployment_cruise_id': self.safe_str_conversion(ds.DEPLOYMENT_CRUISE_ID.values) if 'DEPLOYMENT_CRUISE_ID' in ds else \"\"\n",
    "                    },\n",
    "                    'technical_info': {\n",
    "                        'battery_type': self.safe_str_conversion(ds.BATTERY_TYPE.values) if 'BATTERY_TYPE' in ds else \"\",\n",
    "                        'battery_packs': self.safe_str_conversion(ds.BATTERY_PACKS.values) if 'BATTERY_PACKS' in ds else \"\",\n",
    "                        'controller_board_type_primary': self.safe_str_conversion(ds.CONTROLLER_BOARD_TYPE_PRIMARY.values) if 'CONTROLLER_BOARD_TYPE_PRIMARY' in ds else \"\",\n",
    "                        'firmware_version': self.safe_str_conversion(ds.FIRMWARE_VERSION.values),\n",
    "                        'sensors': []\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                # Extract sensor information\n",
    "                if 'SENSOR' in ds:\n",
    "                    sensors = ds.SENSOR.values\n",
    "                    sensor_makers = ds.SENSOR_MAKER.values if 'SENSOR_MAKER' in ds else []\n",
    "                    sensor_models = ds.SENSOR_MODEL.values if 'SENSOR_MODEL' in ds else []\n",
    "                    sensor_serials = ds.SENSOR_SERIAL_NO.values if 'SENSOR_SERIAL_NO' in ds else []\n",
    "                    \n",
    "                    for i, sensor in enumerate(sensors):\n",
    "                        sensor_info = {\n",
    "                            'name': self.safe_str_conversion(sensor),\n",
    "                            'maker': self.safe_str_conversion(sensor_makers[i]) if i < len(sensor_makers) else \"\",\n",
    "                            'model': self.safe_str_conversion(sensor_models[i]) if i < len(sensor_models) else \"\",\n",
    "                            'serial_no': self.safe_str_conversion(sensor_serials[i]) if i < len(sensor_serials) else \"\"\n",
    "                        }\n",
    "                        meta_info['technical_info']['sensors'].append(sensor_info)\n",
    "                \n",
    "                # Determine location\n",
    "                lat = meta_info['launch_info']['latitude']\n",
    "                lon = meta_info['launch_info']['longitude']\n",
    "                meta_info['location'] = self.determine_location(lat, lon)\n",
    "                \n",
    "                return meta_info\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing meta file {file_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def process_profile_files(self, float_dir, float_id):\n",
    "        \"\"\"Process all profile files (prof.nc and cycle files) for a float\"\"\"\n",
    "        all_data = {\n",
    "            'temp': [], 'psal': [], 'pres': [], 'doxy': [],\n",
    "            'fluorescence_chla': [], 'bbp700': [], 'nitrate': [],\n",
    "            'ph': [], 'turbidity': [], 'cdom': []\n",
    "        }\n",
    "        cycle_count = 0\n",
    "        \n",
    "        # Process main profile file\n",
    "        prof_file = float_dir / f\"{float_id}_prof.nc\"\n",
    "        if prof_file.exists():\n",
    "            try:\n",
    "                with xr.open_dataset(prof_file) as ds:\n",
    "                    cycle_count += len(ds.N_PROF) if 'N_PROF' in ds.dims else 0\n",
    "                    \n",
    "                    # Extract data for each parameter\n",
    "                    for param in ['TEMP', 'PSAL', 'PRES']:\n",
    "                        if param in ds:\n",
    "                            data = ds[param].values\n",
    "                            all_data[param.lower()].extend(data.flatten())\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing profile file {prof_file}: {e}\")\n",
    "        \n",
    "        # Process cycle files (e.g., *_001.nc, *_002.nc, etc.)\n",
    "        cycle_files = list(float_dir.glob(f\"{float_id}_[0-9][0-9][0-9].nc\"))\n",
    "        cycle_count += len(cycle_files)\n",
    "        \n",
    "        for cycle_file in cycle_files:\n",
    "            try:\n",
    "                with xr.open_dataset(cycle_file) as ds:\n",
    "                    # Map of parameter names to our keys\n",
    "                    param_mapping = {\n",
    "                        'TEMP': 'temp',\n",
    "                        'PSAL': 'psal', \n",
    "                        'PRES': 'pres',\n",
    "                        'DOXY': 'doxy',\n",
    "                        'FLUORESCENCE_CHLA': 'fluorescence_chla',\n",
    "                        'BBP700': 'bbp700',\n",
    "                        'NITRATE': 'nitrate',\n",
    "                        'PH_IN_SITU_TOTAL': 'ph',\n",
    "                        'TURBIDITY': 'turbidity',\n",
    "                        'CDOM': 'cdom'\n",
    "                    }\n",
    "                    \n",
    "                    for nc_param, key in param_mapping.items():\n",
    "                        if nc_param in ds:\n",
    "                            data = ds[nc_param].values\n",
    "                            all_data[key].extend(data.flatten())\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing cycle file {cycle_file}: {e}\")\n",
    "        \n",
    "        # Calculate statistics for each parameter\n",
    "        stats = {}\n",
    "        for param, data in all_data.items():\n",
    "            min_val, max_val, avg_val = self.calculate_stats(data)\n",
    "            stats[f'{param}_min'] = min_val\n",
    "            stats[f'{param}_max'] = max_val\n",
    "            stats[f'{param}_avg'] = avg_val\n",
    "        \n",
    "        return stats, cycle_count\n",
    "    \n",
    "    def process_float(self, float_dir):\n",
    "        \"\"\"Process all files for a single float\"\"\"\n",
    "        float_id = float_dir.name\n",
    "        print(f\"Processing float: {float_id}\")\n",
    "        \n",
    "        # Initialize float data structure\n",
    "        float_info = {\n",
    "            \"platform_number\": float_id,\n",
    "            \"wmo_inst_type\": \"\",\n",
    "            \"project_name\": \"\",\n",
    "            \"pi_name\": \"\",\n",
    "            \"data_centre\": \"\",\n",
    "            \"launch_info\": {\n",
    "                \"date\": \"\",\n",
    "                \"latitude\": float('nan'),\n",
    "                \"longitude\": float('nan'),\n",
    "                \"platform_type\": \"\",\n",
    "                \"float_serial_no\": \"\",\n",
    "                \"deployment_platform\": \"\",\n",
    "                \"deployment_cruise_id\": \"\"\n",
    "            },\n",
    "            \"location\": \"\",\n",
    "            \"temp_max\": float('nan'),\n",
    "            \"temp_min\": float('nan'),\n",
    "            \"temp_avg\": float('nan'),\n",
    "            \"psal_max\": float('nan'),\n",
    "            \"psal_min\": float('nan'),\n",
    "            \"psal_avg\": float('nan'),\n",
    "            \"pres_max\": float('nan'),\n",
    "            \"pres_min\": float('nan'),\n",
    "            \"pres_avg\": float('nan'),\n",
    "            \"doxy_max\": float('nan'),\n",
    "            \"doxy_min\": float('nan'),\n",
    "            \"doxy_avg\": float('nan'),\n",
    "            \"fluorescence_chla_max\": float('nan'),\n",
    "            \"fluorescence_chla_min\": float('nan'),\n",
    "            \"fluorescence_chla_avg\": float('nan'),\n",
    "            \"bbp700_max\": float('nan'),\n",
    "            \"bbp700_min\": float('nan'),\n",
    "            \"bbp700_avg\": float('nan'),\n",
    "            \"nitrate_max\": float('nan'),\n",
    "            \"nitrate_min\": float('nan'),\n",
    "            \"nitrate_avg\": float('nan'),\n",
    "            \"ph_max\": float('nan'),\n",
    "            \"ph_min\": float('nan'),\n",
    "            \"ph_avg\": float('nan'),\n",
    "            \"turbidity_max\": float('nan'),\n",
    "            \"turbidity_min\": float('nan'),\n",
    "            \"turbidity_avg\": float('nan'),\n",
    "            \"cdom_max\": float('nan'),\n",
    "            \"cdom_min\": float('nan'),\n",
    "            \"cdom_avg\": float('nan'),\n",
    "            \"technical_info\": {\n",
    "                \"battery_type\": \"\",\n",
    "                \"battery_packs\": \"\",\n",
    "                \"controller_board_type_primary\": \"\",\n",
    "                \"firmware_version\": \"\",\n",
    "                \"sensors\": []\n",
    "            },\n",
    "            \"cycles\": 0,\n",
    "            \"launch_quality\": \"\",\n",
    "            \"data_source\": \"\",\n",
    "            \"status\": \"\",\n",
    "            \"last_updated\": \"\"\n",
    "        }\n",
    "        \n",
    "        # Process meta file\n",
    "        meta_file = float_dir / f\"{float_id}_meta.nc\"\n",
    "        if meta_file.exists():\n",
    "            meta_info = self.process_meta_file(meta_file, float_id)\n",
    "            if meta_info:\n",
    "                float_info.update(meta_info)\n",
    "        \n",
    "        # Process profile files and calculate statistics\n",
    "        stats, cycle_count = self.process_profile_files(float_dir, float_id)\n",
    "        float_info.update(stats)\n",
    "        float_info['cycles'] = cycle_count\n",
    "        \n",
    "        return float_info\n",
    "    \n",
    "    def process_all_floats(self):\n",
    "        \"\"\"Process all float directories\"\"\"\n",
    "        # Find all float directories\n",
    "        float_dirs = [d for d in self.data_directory.iterdir() \n",
    "                     if d.is_dir() and d.name.isdigit()]\n",
    "        \n",
    "        print(f\"Found {len(float_dirs)} float directories\")\n",
    "        \n",
    "        for float_dir in float_dirs:\n",
    "            try:\n",
    "                float_info = self.process_float(float_dir)\n",
    "                self.float_data[float_dir.name] = float_info\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing float {float_dir.name}: {e}\")\n",
    "        \n",
    "        return self.float_data\n",
    "    \n",
    "    def save_json(self, output_file):\n",
    "        \"\"\"Save processed data to JSON file\"\"\"\n",
    "        # Convert NaN values to null for JSON serialization\n",
    "        def convert_nan_to_null(obj):\n",
    "            if isinstance(obj, dict):\n",
    "                return {k: convert_nan_to_null(v) for k, v in obj.items()}\n",
    "            elif isinstance(obj, list):\n",
    "                return [convert_nan_to_null(item) for item in obj]\n",
    "            elif isinstance(obj, float) and np.isnan(obj):\n",
    "                return None\n",
    "            else:\n",
    "                return obj\n",
    "        \n",
    "        json_data = convert_nan_to_null(self.float_data)\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(json_data, f, indent=2)\n",
    "        \n",
    "        print(f\"Data saved to {output_file}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Set your data directory path here\n",
    "    data_directory = \"/Users/vinithlankireddy/Projects/SIH/example\"  # Change this to your actual data path\n",
    "    output_file = \"argo_floats_data.json\"\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(data_directory):\n",
    "        print(f\"Data directory {data_directory} does not exist!\")\n",
    "        print(\"Please update the data_directory variable with the correct path.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = ArgoFloatProcessor(data_directory)\n",
    "    \n",
    "    # Process all floats\n",
    "    print(\"Starting to process Argo float data...\")\n",
    "    float_data = processor.process_all_floats()\n",
    "    \n",
    "    # Save to JSON\n",
    "    processor.save_json(output_file)\n",
    "    \n",
    "    print(f\"Processing complete! Found {len(float_data)} floats.\")\n",
    "    \n",
    "    # Print summary\n",
    "    for float_id, data in list(float_data.items())[:3]:  # Show first 3 floats as example\n",
    "        print(f\"\\nFloat {float_id}:\")\n",
    "        print(f\"  Location: {data['location']}\")\n",
    "        print(f\"  Cycles: {data['cycles']}\")\n",
    "        print(f\"  Temperature range: {data['temp_min']:.2f} - {data['temp_max']:.2f}\")\n",
    "        print(f\"  Salinity range: {data['psal_min']:.2f} - {data['psal_max']:.2f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b34ecce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to process Argo float data...\n",
      "Found 21 float directories\n",
      "Processing float: 1902767\n",
      "Processing float: 1902677\n",
      "Processing float: 2902114\n",
      "Processing float: 1902670\n",
      "Processing float: 2900229\n",
      "Processing float: 2900228\n",
      "Processing float: 1902671\n",
      "Processing float: 2900226\n",
      "Processing float: 1902676\n",
      "Processing float: 1900121\n",
      "Processing float: 5907092\n",
      "Processing float: 2900232\n",
      "Processing float: 2900233\n",
      "Processing float: 1902673\n",
      "Processing float: 1902674\n",
      "Processing float: 1902675\n",
      "Processing float: 1900122\n",
      "Processing float: 1902672\n",
      "Processing float: 2900230\n",
      "Processing float: 1902669\n",
      "Processing float: 1902785\n",
      "Data saved to argo_floats_data2.json\n",
      "Processing complete! Found 21 floats.\n",
      "\n",
      "Float 1902767:\n",
      "  Location: Indian Ocean\n",
      "  Cycles: 19\n",
      "  Temperature range: 2.71 - 17.28\n",
      "  Salinity range: 34.27 - 35.53\n",
      "\n",
      "Float 1902677:\n",
      "  Location: Indian Ocean\n",
      "  Cycles: 61\n",
      "  Temperature range: 2.37 - 30.86\n",
      "  Salinity range: 33.62 - 35.31\n",
      "\n",
      "Float 2902114:\n",
      "  Location: Bay of Bengal\n",
      "  Cycles: 284\n",
      "  Temperature range: 2.51 - 31.84\n",
      "  Salinity range: 23.69 - 35.20\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ArgoFloatProcessor:\n",
    "    def __init__(self, data_directory):\n",
    "        self.data_directory = Path(data_directory)\n",
    "        self.float_data = {}\n",
    "        \n",
    "    def determine_location(self, lat, lon):\n",
    "        \"\"\"Determine ocean region based on latitude and longitude\"\"\"\n",
    "        if pd.isna(lat) or pd.isna(lon):\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        # Bay of Bengal: roughly 5-22°N, 80-100°E\n",
    "        if 5 <= lat <= 22 and 80 <= lon <= 100:\n",
    "            return \"Bay of Bengal\"\n",
    "        \n",
    "        # Arabian Sea: roughly 0-30°N, 50-80°E\n",
    "        elif 0 <= lat <= 30 and 50 <= lon <= 80:\n",
    "            return \"Arabian Sea\"\n",
    "        \n",
    "        # Indian Ocean: broader region\n",
    "        elif -50 <= lat <= 30 and 20 <= lon <= 120:\n",
    "            return \"Indian Ocean\"\n",
    "        \n",
    "        else:\n",
    "            return \"Other\"\n",
    "    \n",
    "    def safe_float_conversion(self, value):\n",
    "        \"\"\"Safely convert value to float, return NaN if not possible\"\"\"\n",
    "        try:\n",
    "            if pd.isna(value):\n",
    "                return float('nan')\n",
    "            return float(value)\n",
    "        except (ValueError, TypeError):\n",
    "            return float('nan')\n",
    "    \n",
    "    def safe_str_conversion(self, value):\n",
    "        \"\"\"Safely convert value to string\"\"\"\n",
    "        try:\n",
    "            if pd.isna(value):\n",
    "                return \"\"\n",
    "            if isinstance(value, bytes):\n",
    "                return value.decode('utf-8').strip()\n",
    "            return str(value).strip()\n",
    "        except:\n",
    "            return \"\"\n",
    "    \n",
    "    def calculate_stats(self, data):\n",
    "        \"\"\"Calculate min, max, avg from data array\"\"\"\n",
    "        try:\n",
    "            if data is None or len(data) == 0:\n",
    "                return float('nan'), float('nan'), float('nan')\n",
    "            \n",
    "            # Flatten the data if it's multi-dimensional\n",
    "            flat_data = np.array(data).flatten()\n",
    "            \n",
    "            # Remove NaN values\n",
    "            valid_data = flat_data[~np.isnan(flat_data)]\n",
    "            \n",
    "            if len(valid_data) == 0:\n",
    "                return float('nan'), float('nan'), float('nan')\n",
    "            \n",
    "            return float(np.min(valid_data)), float(np.max(valid_data)), float(np.mean(valid_data))\n",
    "        except:\n",
    "            return float('nan'), float('nan'), float('nan')\n",
    "    \n",
    "    def process_meta_file(self, file_path, float_id):\n",
    "        \"\"\"Process meta.nc file\"\"\"\n",
    "        try:\n",
    "            with xr.open_dataset(file_path) as ds:\n",
    "                meta_info = {\n",
    "                    'platform_number': self.safe_str_conversion(ds.PLATFORM_NUMBER.values),\n",
    "                    'wmo_inst_type': self.safe_str_conversion(ds.WMO_INST_TYPE.values),\n",
    "                    'project_name': self.safe_str_conversion(ds.PROJECT_NAME.values),\n",
    "                    'pi_name': self.safe_str_conversion(ds.PI_NAME.values),\n",
    "                    'data_centre': self.safe_str_conversion(ds.DATA_CENTRE.values),\n",
    "                    'launch_info': {\n",
    "                        'date': self.safe_str_conversion(ds.LAUNCH_DATE.values),\n",
    "                        'latitude': self.safe_float_conversion(ds.LAUNCH_LATITUDE.values),\n",
    "                        'longitude': self.safe_float_conversion(ds.LAUNCH_LONGITUDE.values),\n",
    "                        'platform_type': self.safe_str_conversion(ds.PLATFORM_TYPE.values),\n",
    "                        'float_serial_no': self.safe_str_conversion(ds.FLOAT_SERIAL_NO.values),\n",
    "                        'deployment_platform': self.safe_str_conversion(ds.DEPLOYMENT_PLATFORM.values) if 'DEPLOYMENT_PLATFORM' in ds else \"\",\n",
    "                        'deployment_cruise_id': self.safe_str_conversion(ds.DEPLOYMENT_CRUISE_ID.values) if 'DEPLOYMENT_CRUISE_ID' in ds else \"\"\n",
    "                    },\n",
    "                    'technical_info': {\n",
    "                        'battery_type': self.safe_str_conversion(ds.BATTERY_TYPE.values) if 'BATTERY_TYPE' in ds else \"\",\n",
    "                        'battery_packs': self.safe_str_conversion(ds.BATTERY_PACKS.values) if 'BATTERY_PACKS' in ds else \"\",\n",
    "                        'controller_board_type_primary': self.safe_str_conversion(ds.CONTROLLER_BOARD_TYPE_PRIMARY.values) if 'CONTROLLER_BOARD_TYPE_PRIMARY' in ds else \"\",\n",
    "                        'firmware_version': self.safe_str_conversion(ds.FIRMWARE_VERSION.values),\n",
    "                        'sensors': []\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                # Extract sensor information\n",
    "                if 'SENSOR' in ds:\n",
    "                    sensors = ds.SENSOR.values\n",
    "                    sensor_makers = ds.SENSOR_MAKER.values if 'SENSOR_MAKER' in ds else []\n",
    "                    sensor_models = ds.SENSOR_MODEL.values if 'SENSOR_MODEL' in ds else []\n",
    "                    sensor_serials = ds.SENSOR_SERIAL_NO.values if 'SENSOR_SERIAL_NO' in ds else []\n",
    "                    \n",
    "                    for i, sensor in enumerate(sensors):\n",
    "                        sensor_info = {\n",
    "                            'name': self.safe_str_conversion(sensor),\n",
    "                            'maker': self.safe_str_conversion(sensor_makers[i]) if i < len(sensor_makers) else \"\",\n",
    "                            'model': self.safe_str_conversion(sensor_models[i]) if i < len(sensor_models) else \"\",\n",
    "                            'serial_no': self.safe_str_conversion(sensor_serials[i]) if i < len(sensor_serials) else \"\"\n",
    "                        }\n",
    "                        meta_info['technical_info']['sensors'].append(sensor_info)\n",
    "                \n",
    "                # Determine location\n",
    "                lat = meta_info['launch_info']['latitude']\n",
    "                lon = meta_info['launch_info']['longitude']\n",
    "                meta_info['location'] = self.determine_location(lat, lon)\n",
    "                \n",
    "                return meta_info\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing meta file {file_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def process_profile_files(self, float_dir, float_id):\n",
    "        \"\"\"Process all profile files (prof.nc and cycle files) for a float\"\"\"\n",
    "        all_data = {\n",
    "            'temp': [], 'psal': [], 'pres': [], 'doxy': [],\n",
    "            'fluorescence_chla': [], 'bbp700': [], 'nitrate': [],\n",
    "            'ph': [], 'turbidity': [], 'cdom': []\n",
    "        }\n",
    "        max_cycle_number = 0\n",
    "        \n",
    "        # Process main profile file\n",
    "        prof_file = float_dir / f\"{float_id}_prof.nc\"\n",
    "        if prof_file.exists():\n",
    "            try:\n",
    "                with xr.open_dataset(prof_file) as ds:\n",
    "                    # Get maximum cycle number from profile file\n",
    "                    if 'CYCLE_NUMBER' in ds:\n",
    "                        cycle_nums = ds.CYCLE_NUMBER.values\n",
    "                        valid_cycles = cycle_nums[~np.isnan(cycle_nums)]\n",
    "                        if len(valid_cycles) > 0:\n",
    "                            max_cycle_number = max(max_cycle_number, int(np.max(valid_cycles)))\n",
    "                    \n",
    "                    # Extract data for each parameter\n",
    "                    for param in ['TEMP', 'PSAL', 'PRES']:\n",
    "                        if param in ds:\n",
    "                            data = ds[param].values\n",
    "                            all_data[param.lower()].extend(data.flatten())\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing profile file {prof_file}: {e}\")\n",
    "        \n",
    "        # Process cycle files (e.g., *_001.nc, *_002.nc, etc.)\n",
    "        cycle_files = list(float_dir.glob(f\"{float_id}_[0-9][0-9][0-9].nc\"))\n",
    "        \n",
    "        for cycle_file in cycle_files:\n",
    "            try:\n",
    "                with xr.open_dataset(cycle_file) as ds:\n",
    "                    # Get maximum cycle number from cycle file\n",
    "                    if 'CYCLE_NUMBER' in ds:\n",
    "                        cycle_nums = ds.CYCLE_NUMBER.values\n",
    "                        valid_cycles = cycle_nums[~np.isnan(cycle_nums)]\n",
    "                        if len(valid_cycles) > 0:\n",
    "                            max_cycle_number = max(max_cycle_number, int(np.max(valid_cycles)))\n",
    "                    \n",
    "                    # Map of parameter names to our keys\n",
    "                    param_mapping = {\n",
    "                        'TEMP': 'temp',\n",
    "                        'PSAL': 'psal', \n",
    "                        'PRES': 'pres',\n",
    "                        'DOXY': 'doxy',\n",
    "                        'FLUORESCENCE_CHLA': 'fluorescence_chla',\n",
    "                        'BBP700': 'bbp700',\n",
    "                        'NITRATE': 'nitrate',\n",
    "                        'PH_IN_SITU_TOTAL': 'ph',\n",
    "                        'TURBIDITY': 'turbidity',\n",
    "                        'CDOM': 'cdom'\n",
    "                    }\n",
    "                    \n",
    "                    for nc_param, key in param_mapping.items():\n",
    "                        if nc_param in ds:\n",
    "                            data = ds[nc_param].values\n",
    "                            all_data[key].extend(data.flatten())\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing cycle file {cycle_file}: {e}\")\n",
    "        \n",
    "        # Also check trajectory file for cycle numbers\n",
    "        traj_file = float_dir / f\"{float_id}_Rtraj.nc\"\n",
    "        if traj_file.exists():\n",
    "            try:\n",
    "                with xr.open_dataset(traj_file) as ds:\n",
    "                    if 'CYCLE_NUMBER' in ds:\n",
    "                        cycle_nums = ds.CYCLE_NUMBER.values\n",
    "                        valid_cycles = cycle_nums[~np.isnan(cycle_nums)]\n",
    "                        if len(valid_cycles) > 0:\n",
    "                            max_cycle_number = max(max_cycle_number, int(np.max(valid_cycles)))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing trajectory file {traj_file}: {e}\")\n",
    "        \n",
    "        # Calculate statistics for each parameter\n",
    "        stats = {}\n",
    "        for param, data in all_data.items():\n",
    "            min_val, max_val, avg_val = self.calculate_stats(data)\n",
    "            stats[f'{param}_min'] = min_val\n",
    "            stats[f'{param}_max'] = max_val\n",
    "            stats[f'{param}_avg'] = avg_val\n",
    "        \n",
    "        return stats, max_cycle_number\n",
    "    \n",
    "    def process_float(self, float_dir):\n",
    "        \"\"\"Process all files for a single float\"\"\"\n",
    "        float_id = float_dir.name\n",
    "        print(f\"Processing float: {float_id}\")\n",
    "        \n",
    "        # Initialize float data structure\n",
    "        float_info = {\n",
    "            \"platform_number\": float_id,\n",
    "            \"wmo_inst_type\": \"\",\n",
    "            \"project_name\": \"\",\n",
    "            \"pi_name\": \"\",\n",
    "            \"data_centre\": \"\",\n",
    "            \"launch_info\": {\n",
    "                \"date\": \"\",\n",
    "                \"latitude\": float('nan'),\n",
    "                \"longitude\": float('nan'),\n",
    "                \"platform_type\": \"\",\n",
    "                \"float_serial_no\": \"\",\n",
    "                \"deployment_platform\": \"\",\n",
    "                \"deployment_cruise_id\": \"\"\n",
    "            },\n",
    "            \"location\": \"\",\n",
    "            \"temp_max\": float('nan'),\n",
    "            \"temp_min\": float('nan'),\n",
    "            \"temp_avg\": float('nan'),\n",
    "            \"psal_max\": float('nan'),\n",
    "            \"psal_min\": float('nan'),\n",
    "            \"psal_avg\": float('nan'),\n",
    "            \"pres_max\": float('nan'),\n",
    "            \"pres_min\": float('nan'),\n",
    "            \"pres_avg\": float('nan'),\n",
    "            \"doxy_max\": float('nan'),\n",
    "            \"doxy_min\": float('nan'),\n",
    "            \"doxy_avg\": float('nan'),\n",
    "            \"fluorescence_chla_max\": float('nan'),\n",
    "            \"fluorescence_chla_min\": float('nan'),\n",
    "            \"fluorescence_chla_avg\": float('nan'),\n",
    "            \"bbp700_max\": float('nan'),\n",
    "            \"bbp700_min\": float('nan'),\n",
    "            \"bbp700_avg\": float('nan'),\n",
    "            \"nitrate_max\": float('nan'),\n",
    "            \"nitrate_min\": float('nan'),\n",
    "            \"nitrate_avg\": float('nan'),\n",
    "            \"ph_max\": float('nan'),\n",
    "            \"ph_min\": float('nan'),\n",
    "            \"ph_avg\": float('nan'),\n",
    "            \"turbidity_max\": float('nan'),\n",
    "            \"turbidity_min\": float('nan'),\n",
    "            \"turbidity_avg\": float('nan'),\n",
    "            \"cdom_max\": float('nan'),\n",
    "            \"cdom_min\": float('nan'),\n",
    "            \"cdom_avg\": float('nan'),\n",
    "            \"technical_info\": {\n",
    "                \"battery_type\": \"\",\n",
    "                \"battery_packs\": \"\",\n",
    "                \"controller_board_type_primary\": \"\",\n",
    "                \"firmware_version\": \"\",\n",
    "                \"sensors\": []\n",
    "            },\n",
    "            \"cycles\": 0,\n",
    "            \"launch_quality\": \"\",\n",
    "            \"data_source\": \"\",\n",
    "            \"status\": \"\",\n",
    "            \"last_updated\": \"\"\n",
    "        }\n",
    "        \n",
    "        # Process meta file\n",
    "        meta_file = float_dir / f\"{float_id}_meta.nc\"\n",
    "        if meta_file.exists():\n",
    "            meta_info = self.process_meta_file(meta_file, float_id)\n",
    "            if meta_info:\n",
    "                float_info.update(meta_info)\n",
    "        \n",
    "        # Process profile files and calculate statistics\n",
    "        stats, max_cycle_number = self.process_profile_files(float_dir, float_id)\n",
    "        float_info.update(stats)\n",
    "        float_info['cycles'] = max_cycle_number\n",
    "        \n",
    "        return float_info\n",
    "    \n",
    "    def process_all_floats(self):\n",
    "        \"\"\"Process all float directories\"\"\"\n",
    "        # Find all float directories\n",
    "        float_dirs = [d for d in self.data_directory.iterdir() \n",
    "                     if d.is_dir() and d.name.isdigit()]\n",
    "        \n",
    "        print(f\"Found {len(float_dirs)} float directories\")\n",
    "        \n",
    "        for float_dir in float_dirs:\n",
    "            try:\n",
    "                float_info = self.process_float(float_dir)\n",
    "                self.float_data[float_dir.name] = float_info\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing float {float_dir.name}: {e}\")\n",
    "        \n",
    "        return self.float_data\n",
    "    \n",
    "    def save_json(self, output_file):\n",
    "        \"\"\"Save processed data to JSON file\"\"\"\n",
    "        # Convert NaN values to null for JSON serialization\n",
    "        def convert_nan_to_null(obj):\n",
    "            if isinstance(obj, dict):\n",
    "                return {k: convert_nan_to_null(v) for k, v in obj.items()}\n",
    "            elif isinstance(obj, list):\n",
    "                return [convert_nan_to_null(item) for item in obj]\n",
    "            elif isinstance(obj, float) and np.isnan(obj):\n",
    "                return None\n",
    "            else:\n",
    "                return obj\n",
    "        \n",
    "        json_data = convert_nan_to_null(self.float_data)\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(json_data, f, indent=2)\n",
    "        \n",
    "        print(f\"Data saved to {output_file}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Set your data directory path here\n",
    "    data_directory = \"/Users/vinithlankireddy/Projects/SIH/example\"  # Change this to your actual data path\n",
    "    output_file = \"argo_floats_data2.json\"\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(data_directory):\n",
    "        print(f\"Data directory {data_directory} does not exist!\")\n",
    "        print(\"Please update the data_directory variable with the correct path.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = ArgoFloatProcessor(data_directory)\n",
    "    \n",
    "    # Process all floats\n",
    "    print(\"Starting to process Argo float data...\")\n",
    "    float_data = processor.process_all_floats()\n",
    "    \n",
    "    # Save to JSON\n",
    "    processor.save_json(output_file)\n",
    "    \n",
    "    print(f\"Processing complete! Found {len(float_data)} floats.\")\n",
    "    \n",
    "    # Print summary\n",
    "    for float_id, data in list(float_data.items())[:3]:  # Show first 3 floats as example\n",
    "        print(f\"\\nFloat {float_id}:\")\n",
    "        print(f\"  Location: {data['location']}\")\n",
    "        print(f\"  Cycles: {data['cycles']}\")\n",
    "        print(f\"  Temperature range: {data['temp_min']:.2f} - {data['temp_max']:.2f}\")\n",
    "        print(f\"  Salinity range: {data['psal_min']:.2f} - {data['psal_max']:.2f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b47794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Updated JSON saved to /Users/vinithlankireddy/Projects/SIH/argo_floats_data2.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "\n",
    "def update_status(json_file, data_dir, output_file=None):\n",
    "\n",
    "    json_file = Path(json_file)\n",
    "    data_dir = Path(data_dir)\n",
    "\n",
    "    # Load JSON\n",
    "    with open(json_file, \"r\") as f:\n",
    "        float_data = json.load(f)\n",
    "\n",
    "    for float_id, info in float_data.items():\n",
    "        meta_file = next(data_dir.glob(f\"{float_id}/*_meta.nc\"), None)\n",
    "\n",
    "        if meta_file and meta_file.exists():\n",
    "            try:\n",
    "                ds = xr.open_dataset(meta_file)\n",
    "                end_mission_date = ds.get(\"END_MISSION_DATE\")\n",
    "\n",
    "                if end_mission_date is not None:\n",
    "                    value = str(end_mission_date.values).strip()\n",
    "\n",
    "                    if value == \"b'              '\":\n",
    "                        info[\"status\"] = \"active\"\n",
    "                    else:\n",
    "                        info[\"status\"] = \"inactive\"\n",
    "                else:\n",
    "                    info[\"status\"] = \"inactive\"  # if variable doesn't exist\n",
    "                ds.close()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {meta_file}: {e}\")\n",
    "                info[\"status\"] = \"inactive\"\n",
    "        else:\n",
    "            print(f\"No meta file found for float {float_id}\")\n",
    "            info[\"status\"] = \"inactive\"\n",
    "\n",
    "    # Save updated JSON\n",
    "    out_path = output_file if output_file else json_file\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(float_data, f, indent=2)\n",
    "\n",
    "    print(f\"✅ Updated JSON saved to {out_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "update_status(\"/Users/vinithlankireddy/Projects/SIH/argo_floats_data2.json\", \"/Users/vinithlankireddy/Projects/SIH/example\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09604c47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
